#!/usr/bin/env python3
"""
Simple script to collect evaluation results from results.json files generated by metrics.py.
Searches for results.json files in output/*/ pattern and reports results
for each scene separately as well as averages across all scenes.
"""

import os
import glob
import json
import argparse
from pathlib import Path
from collections import defaultdict
import numpy as np


def find_results_files(base_dir):
    """Find all results.json files matching the pattern output/dataset/scene/results.json"""
    pattern = os.path.join(base_dir, "output", "*", "*", "results.json")
    # pattern = os.path.join(base_dir, "output", "Instance_1", "*", "results.json")

    results_files = glob.glob(pattern)
    return sorted(results_files)


def extract_scene_name(filepath):
    """Extract dataset and scene name from the file path"""
    parts = Path(filepath).parts
    
    # Find 'output' in the path
    try:
        output_idx = parts.index('output')
        if output_idx + 2 < len(parts):
            dataset = parts[output_idx + 1]
            scene = parts[output_idx + 2]
            return f"{dataset}/{scene}"
    except (ValueError, IndexError):
        pass
    
    return None


def load_scene_metrics(results_files):
    """Load metrics from all results.json files"""
    scene_metrics = {}
    
    for filepath in results_files:
        scene = extract_scene_name(filepath)
        if not scene:
            print(f"Warning: Could not extract scene name from {filepath}")
            continue
            
        try:
            with open(filepath, 'r') as f:
                data = json.load(f)
            scene_metrics[scene] = data
            print(f"Loaded metrics for scene: {scene}")
        except Exception as e:
            print(f"Error loading {filepath}: {e}")
    
    return scene_metrics


def calculate_averages(scene_metrics):
    """Calculate average metrics across all scenes and methods"""
    all_metrics = defaultdict(list)
    method_metrics = defaultdict(lambda: defaultdict(list))
    
    for scene, methods in scene_metrics.items():
        for method, metrics in methods.items():
            for metric_name, value in metrics.items():
                if isinstance(value, (int, float)):
                    all_metrics[metric_name].append(value)
                    method_metrics[method][metric_name].append(value)
    
    # Calculate overall averages
    overall_avg = {}
    for metric_name, values in all_metrics.items():
        if values:
            overall_avg[metric_name] = np.mean(values)
    
    # Calculate per-method averages
    method_avg = {}
    for method, metrics in method_metrics.items():
        method_avg[method] = {}
        for metric_name, values in metrics.items():
            if values:
                method_avg[method][metric_name] = np.mean(values)
    
    return overall_avg, method_avg


def print_results(scene_metrics, overall_avg, method_avg, output_file=None):
    """Print formatted results to console and optionally to file"""
    
    def print_line(line="", file=None):
        print(line)
        if file:
            file.write(line + '\n')
    
    output_f = None
    if output_file:
        output_f = open(output_file, 'w')
    
    try:
        print_line("=" * 80, output_f)
        print_line("GAUSSIAN SPLATTING EVALUATION RESULTS", output_f)
        print_line("=" * 80, output_f)
        print_line(file=output_f)
        
        # Print per-scene results
        print_line("PER-SCENE RESULTS:", output_f)
        print_line("-" * 40, output_f)
        
        for scene in sorted(scene_metrics.keys()):
            print_line(f"\nðŸ“ Scene: {scene}", output_f)
            methods = scene_metrics[scene]
            
            for method in sorted(methods.keys()):
                metrics = methods[method]
                print_line(f"  ðŸ”§ Method: {method}", output_f)
                
                # Print metrics in a consistent order
                metric_order = ['PSNR', 'SSIM', 'LPIPS']
                for metric_name in metric_order:
                    if metric_name in metrics:
                        value = metrics[metric_name]
                        if metric_name == 'PSNR':
                            print_line(f"    ðŸ“Š {metric_name}:  {value:.4f} dB", output_f)
                        else:
                            print_line(f"    ðŸ“Š {metric_name}:  {value:.4f}", output_f)
                
                # Print any additional metrics not in the standard order
                for metric_name, value in sorted(metrics.items()):
                    if metric_name not in metric_order and isinstance(value, (int, float)):
                        print_line(f"    ðŸ“Š {metric_name}:  {value:.4f}", output_f)
        
        # Print method averages
        if method_avg:
            print_line("\n" + "=" * 80, output_f)
            print_line("AVERAGE RESULTS BY METHOD:", output_f)
            print_line("-" * 40, output_f)
            
            for method in sorted(method_avg.keys()):
                metrics = method_avg[method]
                print_line(f"\nðŸ”§ Method: {method}", output_f)
                
                metric_order = ['PSNR', 'SSIM', 'LPIPS']
                for metric_name in metric_order:
                    if metric_name in metrics:
                        value = metrics[metric_name]
                        if metric_name == 'PSNR':
                            print_line(f"  ðŸ“ˆ Avg {metric_name}:  {value:.4f} dB", output_f)
                        else:
                            print_line(f"  ðŸ“ˆ Avg {metric_name}:  {value:.4f}", output_f)
                
                # Print any additional metrics
                for metric_name, value in sorted(metrics.items()):
                    if metric_name not in metric_order:
                        print_line(f"  ðŸ“ˆ Avg {metric_name}:  {value:.4f}", output_f)
        
        # Print overall averages
        if overall_avg:
            print_line("\n" + "=" * 80, output_f)
            print_line("OVERALL AVERAGES (ALL SCENES & METHODS):", output_f)
            print_line("-" * 40, output_f)
            
            metric_order = ['PSNR', 'SSIM', 'LPIPS']
            for metric_name in metric_order:
                if metric_name in overall_avg:
                    value = overall_avg[metric_name]
                    if metric_name == 'PSNR':
                        print_line(f"ðŸ† Average {metric_name}:  {value:.4f} dB", output_f)
                    else:
                        print_line(f"ðŸ† Average {metric_name}:  {value:.4f}", output_f)
            
            # Print any additional metrics
            for metric_name, value in sorted(overall_avg.items()):
                if metric_name not in metric_order:
                    print_line(f"ðŸ† Average {metric_name}:  {value:.4f}", output_f)
        
        # Print summary statistics
        print_line("\n" + "=" * 80, output_f)
        print_line("SUMMARY STATISTICS:", output_f)
        print_line("-" * 40, output_f)
        print_line(f"ðŸ“Š Total scenes processed: {len(scene_metrics)}", output_f)
        
        all_methods = set()
        total_evaluations = 0
        for methods in scene_metrics.values():
            all_methods.update(methods.keys())
            total_evaluations += len(methods)
        
        print_line(f"ðŸ”§ Unique methods found: {len(all_methods)} ({', '.join(sorted(all_methods))})", output_f)
        print_line(f"ðŸ“ˆ Total evaluations: {total_evaluations}", output_f)
        
        print_line("\n" + "=" * 80, output_f)
        
    finally:
        if output_f:
            output_f.close()


def create_csv_summary(scene_metrics, csv_file):
    """Create a CSV summary of all results"""
    import csv
    
    # Collect all unique methods and metrics
    all_methods = set()
    all_metrics = set()
    
    for methods in scene_metrics.values():
        all_methods.update(methods.keys())
        for method_metrics in methods.values():
            all_metrics.update(method_metrics.keys())
    
    all_methods = sorted(all_methods)
    all_metrics = sorted(all_metrics)
    
    with open(csv_file, 'w', newline='') as f:
        writer = csv.writer(f)
        
        # Write header
        header = ['Scene']
        for method in all_methods:
            for metric in all_metrics:
                header.append(f"{method}_{metric}")
        writer.writerow(header)
        
        # Write data
        for scene in sorted(scene_metrics.keys()):
            row = [scene]
            methods = scene_metrics[scene]
            
            for method in all_methods:
                for metric in all_metrics:
                    if method in methods and metric in methods[method]:
                        row.append(methods[method][metric])
                    else:
                        row.append('')
            
            writer.writerow(row)
    
    print(f"CSV summary saved to {csv_file}")


def main():
    parser = argparse.ArgumentParser(description="Collect Gaussian Splatting evaluation results from results.json files")
    parser.add_argument("--base_dir", "-d", default=".", 
                       help="Base directory to search for results files (default: current directory)")
    parser.add_argument("--output_txt", "-o", 
                       help="Save formatted results to text file")
    parser.add_argument("--output_csv", "-c", 
                       help="Save results to CSV file")
    parser.add_argument("--output_json", "-j", 
                       help="Save combined results to JSON file")
    
    args = parser.parse_args()
    
    # Find all results files
    results_files = find_results_files(args.base_dir)
    
    if not results_files:
        print(f"No results.json files found in pattern: {args.base_dir}/output/*/*/results.json")
        print(f"Make sure you have run the evaluation script (metrics.py) first!")
        print(f"Expected structure: output/dataset/scene/results.json")
        return
    
    print(f"Found {len(results_files)} results files:")
    for f in results_files:
        print(f"  ðŸ“„ {f}")
    print()
    
    # Load all metrics
    scene_metrics = load_scene_metrics(results_files)
    
    if not scene_metrics:
        print("No valid metrics found!")
        return
    
    # Calculate averages
    overall_avg, method_avg = calculate_averages(scene_metrics)
    
    # Print results
    print_results(scene_metrics, overall_avg, method_avg, args.output_txt)
    
    # Save additional formats if requested
    if args.output_csv:
        create_csv_summary(scene_metrics, args.output_csv)
    
    if args.output_json:
        combined_results = {
            'scene_metrics': scene_metrics,
            'method_averages': method_avg,
            'overall_averages': overall_avg,
            'summary': {
                'total_scenes': len(scene_metrics),
                'methods': list(set(method for methods in scene_metrics.values() for method in methods.keys())),
                'total_evaluations': sum(len(methods) for methods in scene_metrics.values())
            }
        }
        
        with open(args.output_json, 'w') as f:
            json.dump(combined_results, f, indent=2)
        
        print(f"JSON results saved to {args.output_json}")


if __name__ == "__main__":
    main()